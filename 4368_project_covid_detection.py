# -*- coding: utf-8 -*-
"""4368 Project - Covid Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dDaCfsh4_jWmxOf9Mh8A6GsTG_7qKloz

#Initilization
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("praveengovi/coronahack-chest-xraydataset")

print("Path to dataset files:", path)

import os

data_path = '/root/.cache/kagglehub/datasets/praveengovi/coronahack-chest-xraydataset/versions/3'
for root, dirs, files in os.walk(data_path):
    print("Directory:", root)
    for name in dirs:
        print(" - Subfolder:", name)
    for name in files[:5]:
        print(" - File:", name)
    break

import pandas as pd

# Paths
base_path = '/root/.cache/kagglehub/datasets/praveengovi/coronahack-chest-xraydataset/versions/3'
img_dir = os.path.join(base_path, 'Coronahack-Chest-XRay-Dataset')
csv_path = os.path.join(base_path, 'Chest_xray_Corona_Metadata.csv')

# Load metadata
df = pd.read_csv(csv_path)
print("Total samples:", len(df))
df.head()

print(df['Label'].unique())

print(df['Label'].value_counts())

import os
import pandas as pd
from sklearn.model_selection import train_test_split

# for reference later

# Step 1: Paths
base_path = '/root/.cache/kagglehub/datasets/praveengovi/coronahack-chest-xraydataset/versions/3'
img_dir = os.path.join(base_path, 'Coronahack-Chest-XRay-Dataset')
csv_path = os.path.join(base_path, 'Chest_xray_Corona_Metadata.csv')

# Step 2: Load Metadata
df = pd.read_csv(csv_path)
print("Original label values:", df['Label'].unique())

# Step 3: Filter Only 'Normal' and 'Pnemonia'
df_filtered = df[df['Label'].isin(['Normal', 'Pnemonia'])].copy()
df_filtered['Label'] = df_filtered['Label'].map({'Normal': 0, 'Pnemonia': 1})

print("\nLabel distribution before image path assignment:")
print(df_filtered['Label'].value_counts())

# Step 4: Map All Image Files Recursively
all_image_paths = {}
for root, _, files in os.walk(img_dir):
    for f in files:
        all_image_paths[f] = os.path.join(root, f)

# Step 5: Assign Full Path to Each Row Based on Image Name
df_filtered['image_path'] = df_filtered['X_ray_image_name'].map(all_image_paths)

# Step 6: Remove Rows Without Found Image
before = len(df_filtered)
df_filtered = df_filtered[df_filtered['image_path'].notna()]
after = len(df_filtered)
print(f"\nRemoved {before - after} rows due to missing image files.")

print("\nFinal label distribution:")
print(df_filtered['Label'].value_counts())

# Step 7: Train/Test Split
if len(df_filtered) == 0:
    print("ERROR: Filtered dataset is empty. Please check label values or image paths.")
else:
    train_df, val_df = train_test_split(
        df_filtered,
        test_size=0.2,
        stratify=df_filtered['Label'],
        random_state=42
    )
    print(f"\n Train samples: {len(train_df)}")
    print(f" Validation samples: {len(val_df)}")

import numpy as np
from PIL import Image

IMG_SIZE = 128  # can be different if using untrained models

def load_images(df):
    images = []
    labels = []
    for _, row in df.iterrows():
        try:
            img = Image.open(row['image_path']).convert('L')  # Grayscale
            img = img.resize((IMG_SIZE, IMG_SIZE))
            img = np.array(img) / 255.0  # Normalize
            images.append(img)
            labels.append(row['Label'])
        except Exception as e:
            print(f"Error loading image {row['image_path']}: {e}")
            continue
    return np.array(images), np.array(labels)

# Load train/val images
X_train, y_train = load_images(train_df)
X_val, y_val = load_images(val_df)

# Reshape for CNN input (batch, height, width, channels)
X_train = X_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)
X_val = X_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)

print(" X_train shape:", X_train.shape)
print(" y_train shape:", y_train.shape)
print(" X_val shape:", X_val.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
import matplotlib.pyplot as plt

"""# Standard Testing Parameter"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    MaxPooling2D(2, 2),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),

    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # Binary output: Normal or Pneumonia
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary() # not needed, just nice to have a visualization of the data

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""#experiment #1: reducing dense layer size from 64 to 32

"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(32, activation='relu'),  # smaller than before. it was set to 64.
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""#experiment #2: keeping dense at 32, dropout from 0.3 to 0.5

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout


model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train the model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=32
)

# Plot training and validation accuracy/loss
plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()